{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降维\n",
    "1. 在实际问题中，通常可以大大减少特征的数量，从而将棘手的问题转变为易于解决的问题\n",
    "\n",
    "2. 数据降维确实会丢失一些信息（就好比将图像压缩为JPEG会降低其质量一样）​，所以，它虽然能够**加速训练**，但是也会**轻微降低系统性能**。同时它也让流水线更为复杂，维护难度上升。因此，如果训练太慢，你首先应该尝试的还是继续使用原始数据，然后再考虑数据降维\n",
    "\n",
    "3. 不过在某些情况下，降低训练数据的维度可能会滤除掉一些不必要的噪声和细节，从而导致性能更好（但通常来说不会）\n",
    "\n",
    "4. 除了加快训练，降维对于**数据可视化**(或称DataViz)也非常有用。将维度降到两个（或三个）​，就可以在图形上绘制出高维训练集，通过视觉来检测模式，常常可以获得一些十分重要的洞察，比如聚类\n",
    "\n",
    "5. 学习现在最流行的三种数据降维技术：PCA、Kernal PCA以及LLE。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高纬度是反直觉的\n",
    "1. 高维空间中的样本点是大概率呈现稀疏分布的。大多数训练样本之间可能彼此相距非常远。训练集维度越高，过拟合风险越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 降维的主要方法\n",
    "\n",
    "##### 投影\n",
    "\n",
    "##### 流形学习\n",
    "1. 许多降维算法通过对训练实例所在的流形进行建模来工作。这称为流形学习。它依赖于流形假设（也称为流形假说）​，该假设认为大多数现实世界的高维数据集都接近于低维流形。通常这是根据经验观察到的这种假设。\n",
    "\n",
    "    - 假设你正在参加一个舞会，舞会上的人们(训练实例)都在跳舞，他们的舞步(数据点)在舞池(高维数据集)中移动。突然，你注意到所有的舞者都在按照一种特定的模式(流形)移动，比如他们都沿着某种曲线或者按照某种规律旋转。这就是流形学习的原理，它假设大多数现实世界的高维数据集都接近于低维流形，就像舞会上的舞者们都按照某种模式移动一样。这种假设是基于经验观察的，就像你在舞会上观察到所有的舞者都在按照某种模式移动一样。然后，降维算法就会根据这种模式，将高纬的舞池(数据集)降维到低维\n",
    "\n",
    "简而言之，在训练模型之前降低训练集的维度肯定可以加快训练速度，但这并不总是会导致更好或更简单的解决方案，它取决于数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "1. 原理： 它识别最靠近数据的超平面，然后将数据投影到其上\n",
    "\n",
    "2. 选择**保留最大差异性的轴**看起来比较合理，因为它可能比其他两种投影丢失的信息更少。要证明这一选择，还有一种方法，即比较原始数据集与其轴上的投影之间的均方距离，使这个均方距离最小的轴是最合理的选择，也就是实线代表的轴。这也正是PCA背后的简单思想[1]\n",
    "    - <img src=\"./images/PCA.png\">\n",
    "    - [PCA 原理和代码](\"https://blog.csdn.net/MoreAction_/article/details/107463336\")\n",
    "\n",
    "3. 可解释方差比\n",
    "    - 另一个有用的信息是每个主成分的可解释方差比，可以通过explained_variance_ratio_变量来获得：衡量特征重要性\n",
    "\n",
    "4. 选择正确的维度\n",
    "    - 与其任意选择要减小到的维度，不如选择相加足够大的方差部分（例如95%）的维度。当然，如果你是为了数据可视化而降低维度，这种情况下，需要将维度降低到2或3。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "m = 60\n",
    "np.random.seed(42)\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m,3))\n",
    "X[:,0] = np.cos(angles) + np.sin(angles) / 2 + noise + np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04341894, -0.52999465],\n",
       "       [ 1.64846022,  0.39320727],\n",
       "       [ 1.1964929 , -0.30652195],\n",
       "       [-0.19859085, -0.38356235],\n",
       "       [-1.12018262,  0.23118899]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2D[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87236029, 0.11990007])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/landmark/lib/python3.8/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist['data']\n",
    "y = mnist['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一个更好的选择：将n_components设置为0.0到1.0之间的浮点数来表示要保留的方差率，而不是指定要保留的主成分数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一个选择是将可解释方差绘制成维度的函数（简单地用cumsum绘制，见图8-8）​。曲线上通常会出现一个拐点，其中可解释方差会停止快速增大。在这种情况下，你可以看到将维度降低到大约100而不会损失太多的可解释方差"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
